---
output:
  html_document: default
  md_document:
      variant: markdown_strict
---



# 8.1 Machine Learning with R

## 1\. Practice Guide

Here we use random forest as an example.

### 1) preparation

we need to install the following packages:

  1. `dplyr`: manipulate data frame
  1. `randomForest`: build random forest model
  1. `ROCR`: ROC analysis
  1. `GGally`: plot correlation between features

```{r install-package, eval=FALSE}
install.packages(c('dplyr', 'randomForest', 'ROCR', 'GGally'))
```


 

```{r install-internal, include=FALSE}
# install internal packages
cran_pkg <- c('pander', 'dplyr', 'randomForest', 'ROCR', 'GGally')
lapply(cran_pkg, function(x) {if (!(x %in% .packages(T))) install.packages(x)})
```


```{r knitr-setup, include=FALSE}
# I don't know why, but when you run the code in R terminal, you don't need this
loadNamespace('randomForest')

knitr::opts_chunk$set(cache = T)
```


To avoid conflict of function name, in the following code, I will try me best to use `pkg::fun()` instead of `library(pkg)`. 

Before we start, let set the random seed to make our results reproducible:

```{r}
set.seed(0) 
```

### 2) generate data set

We use one of R's built-in data set, `iris`, Edgar Anderson’s Iris Data set. 

The original data set contains observations for four features (sepal length and width, and petal length and width --- all in cm) of 150 flowers of three species (each 50). 

To make things simple, here we only choose two species, `versicolor` and `virginica`.

```{r}
df <- iris[iris$Species != 'setosa', ]
rownames(df) <- NULL;
df$Species <- factor(df$Species)
```

> - The first line selects rows in `iris` whose species is not `setosa`, so only `versicolor` and `virginica` are left.
> - The second lines remvoes row names of `df`, which is not needed.
> - The third line drops factor level of `Species` variable, `randomForest::randomForest()` would complain if you don't do this. (This is a little technical, the orignial `Species` contains three levels, `setosa`, `versicolor` and `virginica`. Although we remove all `setosa` values, the `setosa` level still exists, and now this level contains no values, that would cause `randomForest::randomForest()` to fail . After we call `factor()`, `Species` contains only two levels, both do have values.)

Let's have a look at our data (only part of it is shown, the whole data contains 100 rows):


```{r}
head(df, 3)

tail(df, 3)
```


### 3) divide data set

Before we build the model, we need to divide the data set into training set and testing set. So we can train our model using data in training set, and evalute the model using data in testing set.

Here we randomly assigns 80 percent samples to the training set, and the left 20 percent to the testing set.

```{r divide-data}
nrow_training <- floor(nrow(df) * 0.8)  # Calculate the size of training sets
indexes <- sample(1:nrow(df), nrow_training)  # these rows will be select for training

training <- df[indexes, ] 
testing <- df[-indexes, ]
```

The code seems a little complicated, and it require you to be familiar with the R language. 

Anyway, I will try to use a simple example to explain the core idea:

> - Image your data contains only 5 rows, the 80 percent is 5 * 0.8 = 4 (in that case `nrow_training` is `4`). 
> - Image you decide to choose the 1st, 2nd, 3rd and 5th rows for training  (in that case `indexes` is `c(1, 2, 3, 5)`)
> - Now `training` contains the 1st, 2nd, 3rd and 5th rows of `df` (`[indexes, ]` means to choose these rows)
> - And `testing` contains the 4th row of `df`  (`[-indexes, ]` means not to choose these rows, so only the 4th row is left)

### 4) Build the model

Then we can build a random forest model.

```{r build-model}
rf_classifier = randomForest::randomForest(Species ~ ., training)
```

The code is fairly easy and straightforward: 

> - `Species` is the reponse variable
> - `.` tells that all other variables are features
> - `training` is the data to train the model

Let's have a look at our model

```{r}
rf_classifier
```

### 5) Evaluate the model

After we build the model, we can make prediction on the testing set:

```{r}
predicted_value <- predict(rf_classifier, testing[, -ncol(testing)])
real_value <- testing[[ncol(testing)]]
```

> - `predict()` needs two arguments, the model and a `data.frame` of features. (`-ncol(testing)` means to drop the last column, so `testing[, -ncol(testing)` only contains features)
> - we use `testing[[ncol(testing)]]` to get the last column, i.e, the real value of `Species` in the testing set

```{r}
predicted_value
real_value
```

As you can see, `predicted_value` and `real_value` both contains 20 values, correspond to 20 rows of testing data. Each value tells a row belongs which species, the former is the model' precdiction, the latter is the real case.

I manually reformat the result to make it more clear:

```{r echo=FALSE}
tibble::tibble(predicted_value, real_value) %>% 
    tibble::add_column(correct = predicted_value == real_value) %>% 
    pander::pander()
```

And we can summarise the result into a confusion matrix:

| &nbsp;               | True versicolor | True virginica |
|----------------------|-----------------|----------------|
| Predicted versicolor | 9               | 2              |
| Predicted virginica  | 0               | 9              |

Now we can calculate some statistics:

- sensitivity: 9 / (9+0) = 100%
- specificity: 9 / (9+2) = 82%
- accuracy: (9 + 9)/20 = 90%
<!-- > - mcc (Matthews correlation coefficient): (9\*9 - 0\*2) / (9\*11\*11\*9) = 0.008 > -->




### 6) ROC

Finally, let's draw a ROC curve.

```{r}
probability <- predict(rf_classifier, testing[, -ncol(testing)], type = 'prob')
label <- ifelse(testing[[5]] ==  levels(testing[[5]])[1], 1, 0)

ROCR::prediction(probability[, 1], label) %>% {ROCR::performance(., 'auc')@y.values[[1]]}


label <- testing[[5]] == colnames(probability)[1]
label <- as.integer(label)

prediction <- ROCR::prediction(probability[, 1], label)
```

> - `probability`: for each row, we use the model to predict the probability of it belongs to each species
> - `levels`: we flag `` `r levels(testing[[5]])[1]` `` as `1`, `` `r levels(testing[[5]])[2]` `` as `0`
> - `prediction`: we calculate the ROC

```{r plot-roc, eval=FALSE}
roc <- ROCR::performance(prediction, 'tpr', 'fpr') 
ROCR::plot(roc, main = 'ROC Curve') 
```

> - Plot the ROC using false positive rate (`'fpr'`) as x axis, true positive rate (`'tpr'`) as y axis.

```{r plot-roc, echo=FALSE}
```

Cauculate the AUC

```{r}
ROCR::performance(prediction, 'auc')@y.values[[1]]
```

### 7) Tips and more

#### 7a) feature correlation

Before we build the model, we usually need to examine our data first. A good start is to explore the correlation between features:

```{r message=FALSE}
GGally::ggpairs(df, columns = 1:4, ggplot2::aes(color = Species))
```

```{r feature-importance, eval=FALSE, include=FALSE}
# if you want to see Importance of each feature
randomForest::randomForest(Species ~ ., training, importance = TRUE) %>% randomForest::varImpPlot()
```

### 8) More reading

The code refer [this post](https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/)

For more  machine learning models, you can refer to [these scripts](https://github.com/urluzhi/scripts/tree/master/Rscript/machine_learning):  

- `logistic_regression.R`: Logistic Regression
- `svm.R`: SVM
- `plot_result.R`: Plot your training and testing performance
  
Last but not the least, you can also read _[The `caret` package](http://topepo.github.io/caret)_, a tutorial written in GitBook


## 2\. Homework

* 学习和使用教程中的代码，使用下面的数据，练习Random Forest，在training set上训练，在test set上预测，汇报模型的prediction performance: 包括 accuracy, sensitivity, specificity, roc\_auc等指标，绘制ROC曲线。

  > **作业要求** ：上交一个文档汇报prediction performance，并解释如上指标所代表的意义，附ROC曲线并解释其意义。

We use another R's built-in dataset, `mtcars`, you need to run the following code to contruct the data:

```{r}
df2 <- mtcars
df2$Transmission <- factor(ifelse(df2$am, 'manual', 'automatic'))
df2 <- df2[ , c('disp', 'hp', 'drat', 'wt', 'qsec', 'Transmission')]
```

> - `am` variable stores information of transmission of the car as integer, `1` means "manual", `0` means "automatic". we transform it into a factor and stores it into a new variable, `Transmission`
> - then we select six columns: `disp`, `hp`, `drat`, `wt`, `qsec`, `Transmission`

```{r eval=FALSE}
df2
```
```{r, echo=FALSE}
df2 %>% pander::pander()
```

In this data, we have five features:

1. `disp`: Displacement (cu.in.)
1. `hp`: Gross horsepowe
1. `drat`: Rear axle ratio
1. `wt`: Weight (1000 lbs)
1. `qsec`: 1/4 mile time

And the reponse variable is `Transmission`.




```{r eval=F, include=FALSE}
set.seed(0)  
nrow_training2 <- floor(nrow(df2)/3) * 2
indexes2 <- sample(1:nrow(df2), nrow_training2) 

# Assign the data to the correct sets
training2 <- df2[indexes2, ] 
testing2 <- df2[-indexes2, ]
rf_classifier2 = randomForest::randomForest(Transmission ~ ., training2) %T>% print

probability2 <- predict(rf_classifier2, testing2[, -ncol(testing2)], type = "prob")
label2 <- testing2[[ncol(testing2)]] %>% {ifelse(. ==  levels(.)[1], 1, 0)}

prediction2 <- ROCR::prediction(probability2[, 1], label2)

prediction2 %>% ROCR::performance("tpr", "fpr") %>% ROCR::plot(main = "ROC Curve") 

ROCR::performance(prediction2, 'auc')@y.values[[1]]
```


