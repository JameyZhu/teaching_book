---
output:
  - html_document
  - md_document
---

# 8.1 Machine Learning with R

## 1. Practice Guide

* Some [simple scripts](https://github.com/urluzhi/scripts/tree/master/Rscript/machine_learning) for machine learning:
  * logistic\_regression.R: Logistic Regression
  * RadomForest.R: Random Forest
  * svm.R: SVM
  * plot\_result.R: Plot your training and testing performance
* The [_caret_ package](http://topepo.github.io/caret): a tutorial written in GitBook






We will use Random forest as an example.

## 1. preparation


install needed packages

```{r eval=FALSE}
cran_pkg <- c('magrittr', 'randomForest', 'dplyr', 'ROCR')
lapply(cran_pkg, function(x) {if (!(x %in% .packages(T))) install.packages(x)})
```

```{r include=FALSE}
# install internal packages

cran_pkg <- c('pander', 'randomForest', 'gplots', 'pheatmap', 'scales', 'reshape2', 'RColorBrewer', 'plotrix')
lapply(cran_pkg, function(x) {if (!(x %in% .packages(T))) install.packages(x)})
```


```{r include=FALSE}
loadNamespace('randomForest')

knitr::opts_chunk$set(cache = T)
```


```{r}
library(magrittr)
```


## 2. generate data set

We use Edgar Anderson’s Iris Data set. The original data set contains observations for four features (sepal length and width, and petal length and width – all in cm) of 150 flowers of three species (each 50). To make things simple, here we only choose two species, `versicolor` and `virginica`.

```{r}
df <- iris %>% tibble::as_tibble() %>% dplyr::filter(Species != 'setosa') %>%
    dplyr::mutate(Species = factor(Species))
```

```{r, echo=FALSE}
df %>% pander::pander()
```

Let's explore the correlation between features:

```{r message=FALSE}
GGally::ggpairs(df, columns = 1:4, ggplot2::aes(color = Species))
```


## 3. divide data set

Before we build the model, we need to divide the data set into training set and validation set. So we can train our model using data in training set, and evalute the model using data in validation set.

Here we randomly assigns 80 percent samples to the traing set, and the left 20 percent to the validation set.

```{r}
set.seed(0)   # Set random seed to make results reproducible:

nrow_training <- floor(nrow(df) * 0.8)  # Calculate the size of training sets
indexes <- sample(1:nrow(df), nrow_training)  # these rows will be select for training

# Assign the data to the correct sets
training <- df[indexes, ] 
validation <- df[-indexes, ]
```

## 4. Build & use the model

Then we can perform random forest prediction.

```{r}
rf_classifier = randomForest::randomForest(Species ~ ., training)
```

```{r echo=FALSE}
rf_classifier
```


After we build the model, we can make prediction on the validation set

```{r}
predicted_value <- predict(rf_classifier, validation[, -ncol(validation)])
real_value <- validation[[ncol(validation)]]
```

```{r echo=FALSE}
tibble::tibble(predicted_value, real_value) %>% 
    tibble::add_column(correct = predicted_value == real_value) %>% 
    pander::pander()
```

## 5. Evaluate the model

To do: accuracy, sensitivity, specificity, ppv, mcc等指标

```{r eval=FALSE, include=FALSE}
# if you want to see Importance of each feature
randomForest::randomForest(Species ~ ., training, importance = TRUE) %>% randomForest::varImpPlot()
```

## 6. ROC

Finally, let's draw a ROC curve.

```{r}
probability <- predict(rf_classifier, validation[, -ncol(validation)], type = "prob")
label <- validation[[5]] %>% {ifelse(. ==  levels(.)[1], 1, 0)}

prediction <- ROCR::prediction(probability[, 1], label)

prediction %>% ROCR::performance("tpr", "fpr") %>% ROCR::plot(main = "ROC Curve") 
```

Cauculate the AUC

```{r}
ROCR::performance(prediction, 'auc')@y.values[[1]]
```





## 7. Homework

* 学习和使用教程中的代码，练习Random Forest，在training set上训练，在test set上预测，汇报不同模型的prediction performance: 包括 accuracy, sensitivity, specificity, ppv, mcc, roc\_auc等指标，绘制ROC曲线。

  > **作业要求** ：上交一个文档汇报prediction performance，并解释如上指标所代表的意义，附ROC曲线并解释其意义。


```{r}
df2 <- mtcars %>% tibble::as_tibble() %>%
    dplyr::mutate(Transmission = ifelse(am, 'manual', 'automatic') %>% factor) %>%
    dplyr::select(5, 'Transmission') 
```


```{r, echo=FALSE}
df2 %>% pander::pander()
```


```{r eval=F, include=FALSE}
set.seed(0)  
nrow_training2 <- floor(nrow(df2)/3) * 2
indexes2 <- sample(1:nrow(df2), nrow_training2) 

# Assign the data to the correct sets
training2 <- df2[indexes2, ] 
validation2 <- df2[-indexes2, ]
rf_classifier2 = randomForest::randomForest(Transmission ~ ., training2) %T>% print

probability2 <- predict(rf_classifier2, validation2[, -ncol(validation2)], type = "prob")
label2 <- validation2[[ncol(validation2)]] %>% {ifelse(. ==  levels(.)[1], 1, 0)}

prediction2 <- ROCR::prediction(probability2[, 1], label2)

prediction2 %>% ROCR::performance("tpr", "fpr") %>% ROCR::plot(main = "ROC Curve") 

ROCR::performance(prediction2, 'auc')@y.values[[1]]
```


## 8. reference

https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/
