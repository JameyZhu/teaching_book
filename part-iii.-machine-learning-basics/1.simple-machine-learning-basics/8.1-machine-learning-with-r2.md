# 8.1 Machine Learning with R

## 1. Practice Guide

Here we use random forest as an example.

### 1\) preparation

install needed packages:

1. `magrittr` enable one style of writing R code, refer to

   [here](https://r4ds.had.co.nz/pipes.html#piping-alternatives) for

   why I use that style.

2. `dplyr`: manipulate data frame
3. `randomForest`: build random forest model
4. `ROCR`: ROC analysis
5. `GGally`: plot correlation between features

```text
cran_pkg <- c('magrittr', 'dplyr', 'randomForest', 'ROCR', 'GGally')
lapply(cran_pkg, function(x) {if (!(x %in% .packages(T))) install.packages(x)})
```

* The first line specify needed packages
* the second line install packages which _doesn’t exist_ in your

  system \(`.packages(T)` will list all existing packages in the

  system.\)

To avoid conflict of function name, in the following code, I will try me best to use `pkg::fun()` instead of `library(pkg)`. However `magrittr` is an exception, I have to `library()` it:

```text
library(magrittr)
```

### 2\) generate data set

We use one of R’s built-in data set, `iris`, Edgar Anderson’s Iris Data set.

The original data set contains observations for four features \(sepal length and width, and petal length and width — all in cm\) of 150 flowers of three species \(each 50\).

To make things simple, here we only choose two species, `versicolor` and `virginica`.

```text
df <- iris %>% tibble::as_tibble() %>% 
    dplyr::filter(Species != 'setosa') %>%
    dplyr::mutate(Species = factor(Species))
```

* The frist line turn `iris` into a

  [`tibble`](https://tibble.tidyverse.org/), a modern reimagining of

  the `data.frame`.

* The second line select rows whose species is not `setosa`, so only

  `versicolor` and `virginica` are left.

* The third line drops factor level of `Species` variable,

  `randomForest::randomForest()` would complain if you don’t do this.

  \(This is a little technical, the orignial `Species` contains three

  levels, `setosa`, `versicolor` and `virginica`. Although we remove

  all `setosa` values, the `setosa` level still exists, and now this

  level contains no values, that would cause

  `randomForest::randomForest()` to fail . After we call `factor()`,

  `Species` contains only two levels, both do have values.\)

Let’s have a look at our data \(only part of it is shown, the whole data contains 100 rows\):

| Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species |
| :--- | :--- | :--- | :--- | :--- |
| 7 | 3.2 | 4.7 | 1.4 | versicolor |
| 6.4 | 3.2 | 4.5 | 1.5 | versicolor |
| 6.9 | 3.1 | 4.9 | 1.5 | versicolor |
| 6.5 | 3 | 5.2 | 2 | virginica |
| 6.2 | 3.4 | 5.4 | 2.3 | virginica |
| 5.9 | 3 | 5.1 | 1.8 | virginica |

### 3\) divide data set

Before we build the model, we need to divide the data set into training set and testing set. So we can train our model using data in training set, and evalute the model using data in testing set.

Here we randomly assigns 80 percent samples to the training set, and the left 20 percent to the testing set.

```text
set.seed(0)   # Set random seed to make results reproducible:

nrow_training <- floor(nrow(df) * 0.8)  # Calculate the size of training sets
indexes <- sample(1:nrow(df), nrow_training)  # these rows will be select for training

training <- df[indexes, ] 
testing <- df[-indexes, ]
```

The code seems a little complicated, and it require you to be familiar with the R language.

Anyway, I will try to use a simple example to explain the core idea:

* Image your data contains only 5 rows, the 80 percent is 5 \* 0.8 = 4

  \(here `nrow_training` is `4`\).

* Image you decide to choose the 1st, 2nd, 3rd and 5th rows for

  training \(here `indexes` is `c(1, 2, 3, 5)`\)

* Now `training` contains the 1st, 2nd, 3rd and 5th rows of `df`

  \(`[indexes, ]` means to choose these rows\)

* And `testing` contains the 4th row of `df` \(`[-indexes, ]` means not

  to choose these rows, so only the 4th row is left\)

### 4\) Build the model

Then we can build a random forest model.

```text
rf_classifier = randomForest::randomForest(Species ~ ., training)
```

The code is fairly easy and straightforward:

* `Species` is the reponse variable
* `.` tells that all other variables are features
* `training` is the data to train the model

Let’s have a look at our model

```text
## 
## Call:
##  randomForest(formula = Species ~ ., data = training) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 7.5%
## Confusion matrix:
##            versicolor virginica class.error
## versicolor         38         3  0.07317073
## virginica           3        36  0.07692308
```

### 5\) Evaluate the model

After we build the model, we can make prediction on the testing set:

```text
predicted_value <- predict(rf_classifier, testing[, -ncol(testing)])
real_value <- testing[[ncol(testing)]]
```

The result is as follows:

| predicted\_value | real\_value | correct |
| :--- | :--- | :--- |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |
| versicolor | virginica | FALSE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |
| versicolor | virginica | FALSE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |

We can summarise the result into a confusion matrix:

|  | True versicolor | True virginica |
| :--- | :--- | :--- |
| Predicted versicolor | 9 | 2 |
| Predicted virginica | 0 | 9 |

Now we can calculate some statistics:

* sensitivity: 9 / \(9+0\) = 100%
* specificity: 9 / \(9+2\) = 82%
* accuracy: \(9 + 9\)/20 = 90%

### 6\) ROC

Finally, let’s draw a ROC curve.

```text
probability <- predict(rf_classifier, testing[, -ncol(testing)], type = 'prob')
label <- testing[[5]] %>% {ifelse(. ==  levels(.)[1], 1, 0)}

prediction <- ROCR::prediction(probability[, 1], label)
```

* `probability`: for each row, we use the model to predict the

  probability of it belongs to each species

* `levels`: we flag `versicolor` as `1`, `virginica` as `0`
* `prediction`: we calculate the ROC

```text
prediction %>% ROCR::performance('tpr', 'fpr') %>% ROCR::plot(main = 'ROC Curve') 
```

* Plot the ROC using false positive rate \(`'fpr'`\) as x axis, true

  positive rate \(`'tpr'`\) as y axis.

![](../../.gitbook/assets/plot-roc-1.png)

Cauculate the AUC

```text
ROCR::performance(prediction, 'auc')@y.values[[1]]

## [1] 0.989899
```

### 7\) Tips and more

#### 7a\) feature correlation

Before we build the model, we usually need to examine our data first. A good start is to explore the correlation between features:

```text
GGally::ggpairs(df, columns = 1:4, ggplot2::aes(color = Species))
```

![](../../.gitbook/assets/unnamed-chunk-8-1.png)

### 8\) More reading

The code refer [this post](https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/)

For more machine learning models, you can refer to [these scripts](https://github.com/urluzhi/scripts/tree/master/Rscript/machine_learning):

* `logistic_regression.R`: Logistic Regression
* `svm.R`: SVM
* `plot_result.R`: Plot your training and testing performance

Last but not the least, you can also read [_The_ `caret` _package_](http://topepo.github.io/caret), a tutorial written in GitBook

## 2. Homework

* 学习和使用教程中的代码，使用下面的数据，练习Random Forest，在training set上训练，在test set上预测，汇报模型的prediction performance: 包括 accuracy, sensitivity, specificity, roc\_auc等指标，绘制ROC曲线。

  > **作业要求** ：上交一个文档汇报prediction performance，并解释如上指标所代表的意义，附ROC曲线并解释其意义。

We use another R’s built-in dataset, `mtcars`, you need to run the following code to contruct the data:

```text
library(magrittr)

df2 <- mtcars %>% tibble::as_tibble() %>%
    dplyr::mutate(Transmission = ifelse(am, 'manual', 'automatic') %>% factor) %>%
    dplyr::select(3:7, 'Transmission')
```

As you can see, `Transmission` is the reponse variable, all other variables are features.

| disp | hp | drat | wt | qsec | Transmission |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 160 | 110 | 3.9 | 2.62 | 16.46 | manual |
| 160 | 110 | 3.9 | 2.875 | 17.02 | manual |
| 108 | 93 | 3.85 | 2.32 | 18.61 | manual |
| 258 | 110 | 3.08 | 3.215 | 19.44 | automatic |
| 360 | 175 | 3.15 | 3.44 | 17.02 | automatic |
| 225 | 105 | 2.76 | 3.46 | 20.22 | automatic |
| 360 | 245 | 3.21 | 3.57 | 15.84 | automatic |
| 146.7 | 62 | 3.69 | 3.19 | 20 | automatic |
| 140.8 | 95 | 3.92 | 3.15 | 22.9 | automatic |
| 167.6 | 123 | 3.92 | 3.44 | 18.3 | automatic |
| 167.6 | 123 | 3.92 | 3.44 | 18.9 | automatic |
| 275.8 | 180 | 3.07 | 4.07 | 17.4 | automatic |
| 275.8 | 180 | 3.07 | 3.73 | 17.6 | automatic |
| 275.8 | 180 | 3.07 | 3.78 | 18 | automatic |
| 472 | 205 | 2.93 | 5.25 | 17.98 | automatic |
| 460 | 215 | 3 | 5.424 | 17.82 | automatic |
| 440 | 230 | 3.23 | 5.345 | 17.42 | automatic |
| 78.7 | 66 | 4.08 | 2.2 | 19.47 | manual |
| 75.7 | 52 | 4.93 | 1.615 | 18.52 | manual |
| 71.1 | 65 | 4.22 | 1.835 | 19.9 | manual |
| 120.1 | 97 | 3.7 | 2.465 | 20.01 | automatic |
| 318 | 150 | 2.76 | 3.52 | 16.87 | automatic |
| 304 | 150 | 3.15 | 3.435 | 17.3 | automatic |
| 350 | 245 | 3.73 | 3.84 | 15.41 | automatic |
| 400 | 175 | 3.08 | 3.845 | 17.05 | automatic |
| 79 | 66 | 4.08 | 1.935 | 18.9 | manual |
| 120.3 | 91 | 4.43 | 2.14 | 16.7 | manual |
| 95.1 | 113 | 3.77 | 1.513 | 16.9 | manual |
| 351 | 264 | 4.22 | 3.17 | 14.5 | manual |
| 145 | 175 | 3.62 | 2.77 | 15.5 | manual |
| 301 | 335 | 3.54 | 3.57 | 14.6 | manual |
| 121 | 109 | 4.11 | 2.78 | 18.6 | manual |

