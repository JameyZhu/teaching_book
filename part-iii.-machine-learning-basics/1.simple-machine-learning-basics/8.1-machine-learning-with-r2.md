8.1 Machine Learning with R
===========================

1. Practice Guide
-----------------

Here we use random forest as an example.

### 1) preparation

install needed packages:

-   `magrittr` enable one style of writing R code, refer to
    [here](https://r4ds.had.co.nz/pipes.html#piping-alternatives) for
    why I use that style.
-   `randomForest`: build random forest model
-   `dplyr`: manipulate data frame
-   `ROCR`: ROC analysis
-   `GGally`: plot correlation between features

``` {.r}
cran_pkg <- c('magrittr', 'dplyr', 'randomForest', 'ROCR', 'GGally')
lapply(cran_pkg, function(x) {if (!(x %in% .packages(T))) install.packages(x)})
```

1.  The first line specify needed packages
2.  the second line install packages which *doesn't exist* in your
    system (`.packages(T)` will list existing packages in the system.)

To avoid conflict of function name, in the following code, I will try me
best to use `pkg::fun()` instead of `library(pkg)`. However `magrittr`
is an exception, I have to `library()` it:

``` {.r}
library(magrittr)
```

### 2) generate data set

We use one of R's built-in data set, `iris`, Edgar Anderson's Iris Data
set.

The original data set contains observations for four features (sepal
length and width, and petal length and width --- all in cm) of 150
flowers of three species (each 50).

To make things simple, here we only choose two species, `versicolor` and
`virginica`.

``` {.r}
df <- iris %>% tibble::as_tibble() %>% 
    dplyr::filter(Species != 'setosa') %>%
    dplyr::mutate(Species = factor(Species))
```

-   The frist line turn `iris` into a
    [`tibble`](https://tibble.tidyverse.org/), a modern reimagining of
    the `data.frame`.
-   The second line select rows whose species is not `setosa`, so only
    `versicolor` and `virginica` are left.
-   The third line drops factor level of `Species` variable,
    `randomForest::randomForest()` would complain if you don't do this.
    (This is a little technical, the orignial `Species` contains three
    levels, `setosa`, `versicolor` and `virginica`. Although we remove
    all `setosa` values, the `setosa` level still exists, and now this
    level contains no values, that would cause
    `randomForest::randomForest()` to fail . After we call `factor()`,
    `Species` contains only two levels, both do have values.)

Let's have a look at our data (only part of it is shown, the whole data
contains 100 rows)

  -----------------------------------------------------------------------
   Sepal.Length   Sepal.Width   Petal.Length   Petal.Width     Species
  -------------- ------------- -------------- ------------- -------------
        7             3.2           4.7            1.4       versicolor

       6.4            3.2           4.5            1.5       versicolor

       6.9            3.1           4.9            1.5       versicolor

       6.5             3            5.2             2         virginica

       6.2            3.4           5.4            2.3        virginica

       5.9             3            5.1            1.8        virginica
  -----------------------------------------------------------------------

### 3) divide data set

Before we build the model, we need to divide the data set into training
set and testing set. So we can train our model using data in training
set, and evalute the model using data in testing set.

Here we randomly assigns 80 percent samples to the training set, and the
left 20 percent to the testing set.

``` {.r}
set.seed(0)   # Set random seed to make results reproducible:

nrow_training <- floor(nrow(df) * 0.8)  # Calculate the size of training sets
indexes <- sample(1:nrow(df), nrow_training)  # these rows will be select for training

# Assign the data to the correct sets
training <- df[indexes, ] 
testing <- df[-indexes, ]
```

### 4) Build the model

Then we can build a random forest model.

``` {.r}
rf_classifier = randomForest::randomForest(Species ~ ., training)
```

The code is fairly easy and straightforward:

-   `Species` is the reponse variable
-   `.` tells that all other variables are features
-   `training` is the data to train the model

Let's have a look at our model

    ## 
    ## Call:
    ##  randomForest(formula = Species ~ ., data = training) 
    ##                Type of random forest: classification
    ##                      Number of trees: 500
    ## No. of variables tried at each split: 2
    ## 
    ##         OOB estimate of  error rate: 7.5%
    ## Confusion matrix:
    ##            versicolor virginica class.error
    ## versicolor         38         3  0.07317073
    ## virginica           3        36  0.07692308

### 5) Evaluate the model

After we build the model, we can make prediction on the testing set:

``` {.r}
predicted_value <- predict(rf_classifier, testing[, -ncol(testing)])
real_value <- testing[[ncol(testing)]]
```

The result is as follows:

  ------------------------------------------
   predicted\_value   real\_value   correct
  ------------------ ------------- ---------
      versicolor      versicolor     TRUE

      versicolor      versicolor     TRUE

      versicolor      versicolor     TRUE

      versicolor      versicolor     TRUE

      versicolor      versicolor     TRUE

      versicolor      versicolor     TRUE

      versicolor      versicolor     TRUE

      versicolor      versicolor     TRUE

      versicolor      versicolor     TRUE

      virginica        virginica     TRUE

      virginica        virginica     TRUE

      versicolor       virginica     FALSE

      virginica        virginica     TRUE

      virginica        virginica     TRUE

      virginica        virginica     TRUE

      virginica        virginica     TRUE

      versicolor       virginica     FALSE

      virginica        virginica     TRUE

      virginica        virginica     TRUE

      virginica        virginica     TRUE
  ------------------------------------------

We can summarise the result into a confusion matrix:

                         True versicolor   True virginica
  ---------------------- ----------------- ----------------
  Predicted versicolor   9                 2
  Predicted virginica    0                 9

Now we can calculate some statistics:

-   sensitivity: 9 / (9+0) = 100%
-   specificity: 9 / (9+2) = 82%
-   accuracy: (9 + 9)/20 = 90%
    <!--   - mcc (Matthews correlation coefficient): (9\*9 - 0\*2) / (9\*11\*11\*9) = 0.008   -->

### 6) ROC

Finally, let's draw a ROC curve.

``` {.r}
probability <- predict(rf_classifier, testing[, -ncol(testing)], type = 'prob')
label <- testing[[5]] %>% {ifelse(. ==  levels(.)[1], 1, 0)}

prediction <- ROCR::prediction(probability[, 1], label)


prediction %>% ROCR::performance('tpr', 'fpr') %>% ROCR::plot(main = 'ROC Curve') 
```

![](8.1-machine-learning-with-r2_files/figure-markdown/unnamed-chunk-7-1.png)

-   `probability`: for each row, we use the model to predict the
    probability of it belongs to each species
-   `levels`: we flag `versicolor` as `1`, `virginica` as `0`
-   `prediction`: we calculate the ROC
-   the last line plot the ROC using false positive rate (`'fpr'`) as x
    axis, true positive rate (`'tpr'`) as y axis

Cauculate the AUC

``` {.r}
ROCR::performance(prediction, 'auc')@y.values[[1]]
```

    ## [1] 0.989899

### 7) Tips and more

#### 7a) feature correlation

Before we build the model, we usually need to examine our data first. A
good start is to explore the correlation between features:

``` {.r}
GGally::ggpairs(df, columns = 1:4, ggplot2::aes(color = Species))
```

![](8.1-machine-learning-with-r2_files/figure-markdown/unnamed-chunk-9-1.png)

### 8) More reading

The code refer [this
post](https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/)

For more machine learning models, you can refer to [these
scripts](https://github.com/urluzhi/scripts/tree/master/Rscript/machine_learning):

-   `logistic_regression.R`: Logistic Regression
-   `svm.R`: SVM
-   `plot_result.R`: Plot your training and testing performance

Last but not the least, you can also read *[The `caret`
package](http://topepo.github.io/caret)*, a tutorial written in GitBook

2. Homework
-----------

-   学习和使用教程中的代码，使用下面的数据，练习Random
    Forest，在training set上训练，在test set上预测，汇报模型的prediction
    performance: 包括 accuracy, sensitivity, specificity,
    roc\_auc等指标，绘制ROC曲线。

    > **作业要求** ：上交一个文档汇报prediction
    > performance，并解释如上指标所代表的意义，附ROC曲线并解释其意义。

We use another R's built-in dataset, `mtcars`, you need to run the
following code to contruct the data:

``` {.r}
library(magrittr)

df2 <- mtcars %>% tibble::as_tibble() %>%
    dplyr::mutate(Transmission = ifelse(am, 'manual', 'automatic') %>% factor) %>%
    dplyr::select(3:7, 'Transmission')
```

As you can see, `Transmission` is the reponse variable, all other
variables are features.

  ---------------------------------------------------
   disp    hp    drat    wt     qsec    Transmission
  ------- ----- ------ ------- ------- --------------
    160    110   3.9    2.62    16.46      manual

    160    110   3.9    2.875   17.02      manual

    108    93    3.85   2.32    18.61      manual

    258    110   3.08   3.215   19.44    automatic

    360    175   3.15   3.44    17.02    automatic

    225    105   2.76   3.46    20.22    automatic

    360    245   3.21   3.57    15.84    automatic

   146.7   62    3.69   3.19     20      automatic

   140.8   95    3.92   3.15    22.9     automatic

   167.6   123   3.92   3.44    18.3     automatic

   167.6   123   3.92   3.44    18.9     automatic

   275.8   180   3.07   4.07    17.4     automatic

   275.8   180   3.07   3.73    17.6     automatic

   275.8   180   3.07   3.78     18      automatic

    472    205   2.93   5.25    17.98    automatic

    460    215    3     5.424   17.82    automatic

    440    230   3.23   5.345   17.42    automatic

   78.7    66    4.08    2.2    19.47      manual

   75.7    52    4.93   1.615   18.52      manual

   71.1    65    4.22   1.835   19.9       manual

   120.1   97    3.7    2.465   20.01    automatic

    318    150   2.76   3.52    16.87    automatic

    304    150   3.15   3.435   17.3     automatic

    350    245   3.73   3.84    15.41    automatic

    400    175   3.08   3.845   17.05    automatic

    79     66    4.08   1.935   18.9       manual

   120.3   91    4.43   2.14    16.7       manual

   95.1    113   3.77   1.513   16.9       manual

    351    264   4.22   3.17    14.5       manual

    145    175   3.62   2.77    15.5       manual

    301    335   3.54   3.57    14.6       manual

    121    109   4.11   2.78    18.6       manual
  ---------------------------------------------------
