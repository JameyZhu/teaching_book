<<<<<<< HEAD

# Machine Learning with R

## Practice Guide

我们选用Random Forest模型作为示例讲解机器学习。

### 加载R包

运行代码之前需要安装以下R包

* [randomForest](https://cran.r-project.org/web/packages/randomForest/index.html): 构建Random Forest模型
* [ROCR](https://cran.r-project.org/web/packages/ROCR/index.html): 绘制ROC曲线和计算AUC
* [GGally](https://cran.r-project.org/web/packages/GGally/index.html): 画图表示特征之间相关性
* [mlbench](https://cran.r-project.org/web/packages/mlbench/index.html): 常用机器学习数据集


```R
for(pkg in c('randomForest', 'ROCR', 'GGally', 'mlbench')){
  if(!require(pkg, character.only=TRUE)){
    install.packages(pkg)
  }
}
```

设置随机数种子保证本教程的结果可重复


```R
set.seed(1234) 
```

### 加载数据集

我们采用R内置的数据集`iris`，其中包含4个特征和3个类别。每个类别包含50个样本，对应一个花的物种。数据集一共包含150个样本。


```R
head(iris)
```


<table>
<thead><tr><th scope=col>Sepal.Length</th><th scope=col>Sepal.Width</th><th scope=col>Petal.Length</th><th scope=col>Petal.Width</th><th scope=col>Species</th></tr></thead>
<tbody>
	<tr><td>5.1   </td><td>3.5   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>
	<tr><td>4.9   </td><td>3.0   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>
	<tr><td>4.7   </td><td>3.2   </td><td>1.3   </td><td>0.2   </td><td>setosa</td></tr>
	<tr><td>4.6   </td><td>3.1   </td><td>1.5   </td><td>0.2   </td><td>setosa</td></tr>
	<tr><td>5.0   </td><td>3.6   </td><td>1.4   </td><td>0.2   </td><td>setosa</td></tr>
	<tr><td>5.4   </td><td>3.9   </td><td>1.7   </td><td>0.4   </td><td>setosa</td></tr>
</tbody>
</table>



为简单期间，我们只选用`versicolor`和`virginica`两类做二分类问题。


```R
# 去除类别为setosa的样本
df <- iris[iris$Species != 'setosa', ]
# 去除最后一列Species，产生输入矩阵
all_data <- df[, 1:(ncol(df) - 1)]
# 需要预测的类别向量。factor函数用于把原来的3种类别编程2中类别
all_classes <- factor(df$Species)
```

### 划分训练集和测试集

我们随机选择80%的样本（80个样本）作为训练集，剩余20%的样本作为测试集。


```R
# 总样本数为输入矩阵的行数
n_samples <- nrow(all_data)
# 计算训练集样本数(80%)
n_train <- floor(n_samples * 0.8)  # Calculate the size of training sets
# 对样本进行随机排列，产生排列的顺序
indices = sample(1:n_samples)
# 选择随机排列后的前80%样本作为训练集
train_data <- all_data[indices[1:n_train],]
train_classes <- all_classes[indices[1:n_train]]
# 选择随机排列后的后20%样本作为测试集
test_data <- all_data[indices[(n_train + 1):n_samples],]
test_classes <- all_classes[indices[(n_train + 1):n_samples]]
```

### 模型训练

以下代码在训练集上训练一个由100棵分类树组成的Random Forest模型：


```R
rf_classifier = randomForest(train_data, train_classes, trees = 100, importance = TRUE)
```

函数返回的变量`rf_classifier`包含了已经训练好的模型：


```R
rf_classifier
```


    
    Call:
     randomForest(x = train_data, y = train_classes, importance = TRUE,      trees = 100) 
                   Type of random forest: classification
                         Number of trees: 500
    No. of variables tried at each split: 2
    
            OOB estimate of  error rate: 8.75%
    Confusion matrix:
               versicolor virginica class.error
    versicolor         36         3  0.07692308
    virginica           4        37  0.09756098


### 模型测试和评估

模型训练完成之后，可用`predict`函数在测试集上进行预测：


```R
predicted_classes <- predict(rf_classifier, test_data)
```

用预测的类别和真实类别可构建一个混淆矩阵（confusion matrix）


```R
# 定义versicolor为正类别
positive_class <- 'versicolor'
# true positive count (TP)
TP <- sum((predicted_classes == positive_class) & (test_classes == positive_class))
# false positive count (FP)
FP <- sum((predicted_classes == positive_class) & (test_classes != positive_class))
# false negative count (FN)
FN <- sum((predicted_classes != positive_class) & (test_classes == positive_class))
# true negative count (TN)
TN <- sum((predicted_classes != positive_class) & (test_classes != positive_class))
# 构建2x2矩阵，填充以上计算的四个数
confusion <- matrix(nrow=2, ncol=2)
confusion[1, 1] = TP
confusion[1, 2] = FP
confusion[2, 1] = FN
confusion[2, 2] = TN
colnames(confusion) <- c('True versicolor', 'True virginica')
rownames(confusion) <- c('Predicted versicolor', 'Predicted virginica')
confusion
```


<table>
<thead><tr><th></th><th scope=col>True versicolor</th><th scope=col>True virginica</th></tr></thead>
<tbody>
	<tr><th scope=row>Predicted versicolor</th><td>11</td><td>1 </td></tr>
	<tr><th scope=row>Predicted virginica</th><td> 0</td><td>8 </td></tr>
</tbody>
</table>



关于混淆矩阵的计算，可参考(https://en.wikipedia.org/wiki/Confusion_matrix)获得更多信息。

我们可以基于混淆矩阵计算accuracy、sensitivity、positive predicted value、specificity等评估指标：


```R
print(paste('accuracy =', (TP + TN)/(TP + TN + FP + FN)))
print(paste('sensitivity =', TP/(TP + FN)))
print(paste('positive predicted value =', TP/(TP + FP)))
print(paste('specificity =', TN/(TN + FP)))
```

    [1] "accuracy = 0.95"
    [1] "sensitivity = 1"
    [1] "positive predicted value = 0.916666666666667"
    [1] "specificity = 0.888888888888889"


### ROC曲线

ROC曲线需要两组数据：真实类别和预测某一类别的概率。
首先，调用`predict`函数时加上`type = 'prob'`参数可计算每个类别的概率


```R
predicted_probs <- predict(rf_classifier, test_data, type = 'prob')
```

`predicted_probs`包含呢两列，对应两个类别


```R
head(predicted_probs)
```


<table>
<thead><tr><th></th><th scope=col>versicolor</th><th scope=col>virginica</th></tr></thead>
<tbody>
	<tr><th scope=row>106</th><td>0.000</td><td>1.000</td></tr>
	<tr><th scope=row>107</th><td>0.954</td><td>0.046</td></tr>
	<tr><th scope=row>79</th><td>0.982</td><td>0.018</td></tr>
	<tr><th scope=row>141</th><td>0.000</td><td>1.000</td></tr>
	<tr><th scope=row>80</th><td>1.000</td><td>0.000</td></tr>
	<tr><th scope=row>64</th><td>0.992</td><td>0.008</td></tr>
</tbody>
</table>



因为我们把`vesicolor`当作正样本，所以只选取预测为正样本的概率来计算ROC：


```R
# 创建一个长度与测试集大小相同的0-1向量，1代表要预测的类别
test_labels <- vector('integer', length(test_classes))
test_labels[test_classes == positive_class] <- 1
# 通过prediction函数，使用预测为正样本的概率和真实类别创建一个对象pred
pred <- prediction(predicted_probs[, positive_class], test_labels)
```

以假阳性率（false positive rate, fpr)为X轴, 真阳性率（true positive rate, tpr）为y轴绘制ROC曲线：


```R
roc <- performance(pred, 'tpr', 'fpr') 
plot(roc, main = 'ROC Curve') 
```


![png](8.1-machine-learning-with-r_files/8.1-machine-learning-with-r_28_0.png)


计算ROC曲线下面积（area under the curve, AUC）：


```R
auc <- performance(pred, 'auc')@y.values[[1]]
print(paste('auc =', auc))
```

    [1] "auc = 0.98989898989899"


### 特征之间相关性

在模型训练之前，可以计算特征之间相关性，去除冗余的特征。注意特征数较多时，由于计算量很大，不适合分析所有特征之间的相关性。


```R
GGally::ggpairs(df, columns = 1:4, ggplot2::aes(color = Species))
```




![png](8.1-machine-learning-with-r_files/8.1-machine-learning-with-r_32_1.png)


## More reading

The code refer [this post](https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/)

For more  machine learning models, you can refer to [these scripts](https://github.com/urluzhi/scripts/tree/master/Rscript/machine_learning):  

- `logistic_regression.R`: Logistic Regression
- `svm.R`: SVM
- `plot_result.R`: Plot your training and testing performance
  
Last but not the least, you can also read _[The `caret` package](http://topepo.github.io/caret)_, a tutorial written in GitBook

## Homework

学习和使用教程中的代码，使用下面的数据，练习Random Forest，在training set上训练，在test set上预测，汇报模型的prediction performance: 包括 accuracy, sensitivity, specificity, roc\_auc等指标，绘制ROC曲线。

  > **作业要求** ：上交一个文档汇报prediction performance，并解释如上指标所代表的意义，附ROC曲线并解释其意义。

请采用`mlbench`包中的数据集`BreastCancer`：


```R
data('BreastCancer')
head(BreastCancer)
```


<table>
<thead><tr><th scope=col>Id</th><th scope=col>Cl.thickness</th><th scope=col>Cell.size</th><th scope=col>Cell.shape</th><th scope=col>Marg.adhesion</th><th scope=col>Epith.c.size</th><th scope=col>Bare.nuclei</th><th scope=col>Bl.cromatin</th><th scope=col>Normal.nucleoli</th><th scope=col>Mitoses</th><th scope=col>Class</th></tr></thead>
<tbody>
	<tr><td>1000025  </td><td>5        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>
	<tr><td>1002945  </td><td>5        </td><td>4        </td><td>4        </td><td>5        </td><td>7        </td><td>10       </td><td>3        </td><td>2        </td><td>1        </td><td>benign   </td></tr>
	<tr><td>1015425  </td><td>3        </td><td>1        </td><td>1        </td><td>1        </td><td>2        </td><td>2        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>
	<tr><td>1016277  </td><td>6        </td><td>8        </td><td>8        </td><td>1        </td><td>3        </td><td>4        </td><td>3        </td><td>7        </td><td>1        </td><td>benign   </td></tr>
	<tr><td>1017023  </td><td>4        </td><td>1        </td><td>1        </td><td>3        </td><td>2        </td><td>1        </td><td>3        </td><td>1        </td><td>1        </td><td>benign   </td></tr>
	<tr><td>1017122  </td><td>8        </td><td>10       </td><td>10       </td><td>8        </td><td>7        </td><td>10       </td><td>9        </td><td>7        </td><td>1        </td><td>malignant</td></tr>
</tbody>
</table>



构建模型之前，需要用以下代码产生所需数据集：


```R
# 去除含有缺失值的样本
df <- na.omit(BreastCancer)
# 去除Id和Class两列，产生输入矩阵
all_data <- df[, 2:(ncol(df) - 1)]
# 把Class这一列作为要预测的类别
all_classes <- df$Class
```

输入为9个特征，均为整数。

输出为两个类别，良性（benign）和恶性（malignant）。

以下时各特征的含义：

* `Cl.thickness`: Clump Thickness
* `Cell.size`: Uniformity of Cell Size
* `Cell.shape`: Uniformity of Cell Shape
* `Marg.adhesion`: Marginal Adhesion
* `Epith.c.size`: Single Epithelial Cell Size
* `Bare.nuclei`: Bare Nuclei
* `Bl.cromatin`: Bland Chromatin
* `Normal.nucleoli`: Normal Nucleoli
* `Mitoses`: Mitoses

如需了解更多关于`BreastCancer`数据集信息，可参考[mlbench](https://cran.r-project.org/web/packages/mlbench/index.html)的文档。
=======
# 8.1 Machine Learning with R

## 1. Practice Guide

Here we use **Random Forest** classifier as an example.

### 1\) Prepare R packages

* We need to install the following R packages first:

```r
install.packages(c('randomForest', 'ROCR', 'GGally'))
```

> 1. `randomForest`: build random forest model
> 2. `ROCR`: ROC analysis
> 3. `GGally`: plot correlation between features
>
> To avoid conflict of function name, in the following code, I prefer `pkg::fun()` instead of `library(pkg)`.

* Before we start, let set the random seed to make our results reproducible:

```r
set.seed(0) 
```

### 2\) Generate/Read a data set

We use one of R’s built-in data set, `iris`, Edgar Anderson’s Iris Data set.

The original data set contains observations for four features (sepal length and width, and petal length and width — all in cm) of 150 flowers of three species (each 50).

* To make things simple, here we only choose two species, `versicolor` and `virginica`.

```r
df <- iris[iris$Species != 'setosa', ]
rownames(df) <- NULL;
df$Species <- factor(df$Species)
```

> 1. The first line selects rows in `iris` whose species is not `setosa`, so only `versicolor` and `virginica` are left.
> 2. The second lines remvoes row names of `df`, which is not needed.
> 3. The third line drops factor level of `Species` variable, `randomForest::randomForest()` would complain if you don’t do this. (This is a little technical, the orignial `Species` contains three levels, `setosa`, `versicolor` and `virginica`. Although we remove all `setosa` values, the `setosa` level still exists, and now this level contains no values, that would cause `randomForest::randomForest()` to fail . After we call `factor()`, `Species` contains only two levels, both do have values.)

* Let’s have a look at our data (only part of it is shown, the whole data contains 100 rows):

```r
head(df, 3)
```

```text
##   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
## 1          7.0         3.2          4.7         1.4 versicolor
## 2          6.4         3.2          4.5         1.5 versicolor
## 3          6.9         3.1          4.9         1.5 versicolor
```

```r
tail(df, 3)
```

```text
##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
## 98           6.5         3.0          5.2         2.0 virginica
## 99           6.2         3.4          5.4         2.3 virginica
## 100          5.9         3.0          5.1         1.8 virginica
```

### 3\) Divide the data into training and test sets

Before we build the model, we need to divide the data set into training set and testing set. So we can train our model using data in training set, and evalute the model using data in testing set.

Here we randomly assigns 80 percent samples to the training set, and the left 20 percent to the testing set.

```r
nrow_training <- floor(nrow(df) * 0.8)  # Calculate the size of training sets
indexes <- sample(1:nrow(df), nrow_training)  # these rows will be select for training

training <- df[indexes, ] 
testing <- df[-indexes, ]
```

The code seems a little complicated, and it require you to be familiar with the R language.

Anyway, I will try to use a simple example to explain the core idea:

> * Image your data contains only 5 rows, the 80 percent is 5 \* 0.8 = 4 (in that case `nrow_training` is `4`).
> * Image you decide to choose the 1st, 2nd, 3rd and 5th rows for training (in that case `indexes` is `c(1, 2, 3, 5)`)
> * Now `training` contains the 1st, 2nd, 3rd and 5th rows of `df` (`[indexes, ]` means to choose these rows)
> * And `testing` contains the 4th row of `df` (`[-indexes, ]` means not to choose these rows, so only the 4th row is left)

### 4\) Build the model on training set

Then we can build a random forest model.

```r
rf_classifier = randomForest::randomForest(Species ~ ., training)
```

The code is fairly easy and straightforward:

> * `Species` is the reponse variable
> * `.` tells that all other variables are features
> * `training` is the data to train the model

Let’s have a look at our model

```r
rf_classifier
```

```text
## 
## Call:
##  randomForest(formula = Species ~ ., data = training) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 7.5%
## Confusion matrix:
##            versicolor virginica class.error
## versicolor         38         3  0.07317073
## virginica           3        36  0.07692308
```

### 5\) Evaluate the model on test set

After we build the model, we can make prediction on the testing set:

```r
predicted_value <- predict(rf_classifier, testing[, -ncol(testing)])
real_value <- testing[[ncol(testing)]]
```

> * `predict()` needs two arguments, the model and a `data.frame` of features. (`-ncol(testing)` means to drop the last column, so `testing[, -ncol(testing)` only contains features)
> * we use `testing[[ncol(testing)]]` to get the last column, i.e, the real value of `Species` in the testing set

```r
predicted_value
```

```text
##          8         11         15         18         32         33 
## versicolor versicolor versicolor versicolor versicolor versicolor 
##         44         47         50         53         55         57 
## versicolor versicolor versicolor  virginica  virginica versicolor 
##         65         68         71         73         89         94 
##  virginica  virginica  virginica  virginica versicolor  virginica 
##         97        100 
##  virginica  virginica 
## Levels: versicolor virginica
```

```r
real_value
```

```text
##  [1] versicolor versicolor versicolor versicolor versicolor versicolor
##  [7] versicolor versicolor versicolor virginica  virginica  virginica 
## [13] virginica  virginica  virginica  virginica  virginica  virginica 
## [19] virginica  virginica 
## Levels: versicolor virginica
```

As you can see, `predicted_value` and `real_value` both contains 20 values, correspond to 20 rows of testing data. Each value tells a row belongs which species, the former is the model’ precdiction, the latter is the real case.

I manually reformat the result to make it more clear:

| predicted\_value | real\_value | correct |
| :--- | :--- | :--- |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| versicolor | versicolor | TRUE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |
| versicolor | virginica | FALSE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |
| versicolor | virginica | FALSE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |
| virginica | virginica | TRUE |

And we can summarise the result into a confusion matrix:

|  | True versicolor | True virginica |
| :--- | :--- | :--- |
| Predicted versicolor | 9 | 2 |
| Predicted virginica | 0 | 9 |

Now we can calculate some statistics:

* sensitivity: 9 / \(9+0\) = 100%
* specificity: 9 / \(9+2\) = 82%
* accuracy: \(9 + 9\)/20 = 90%

### 6\) ROC

Finally, let’s draw a ROC curve.

```r
probability <- predict(rf_classifier, testing[, -ncol(testing)], type = 'prob')

label <- testing[[5]] == colnames(probability)[1]
label <- as.integer(label)

prediction <- ROCR::prediction(probability[ , 1], label)
```

> * `probability`: for each row, we use the model to predict the probability of it belongs to each species
> * `label`: we flag the first level (`versicolor`  here) as `1`, the second level (`virginica` here) as `0`. (Notice the `[1]`, and `as.integer()` when convert `T` to `1`, `F` to `2`.)
> * `prediction`: we calculate the ROC statistics. `[ , 1]` chooses the first column, corresponds to the `[1]` when creates `label`.

```r
roc <- ROCR::performance(prediction, 'tpr', 'fpr') 
ROCR::plot(roc, main = 'ROC Curve')
```

> * Plot the ROC using false positive rate (`'fpr'`) as x axis, true positive rate (`'tpr'`) as y axis.

![](../../.gitbook/assets/plot-roc-1.png)

Cauculate the AUC

```r
auc <- ROCR::performance(prediction, 'auc');
auc@y.values[[1]]
```

```text
## [1] 0.989899
```

## 2. Tips 

#### 1\) Feature Correlation

Before we build the model, we usually need to examine our data first. A good start is to explore the correlation between features:

```r
GGally::ggpairs(df, columns = 1:4, ggplot2::aes(color = Species))
```

![](../../.gitbook/assets/unnamed-chunk-10-1.png)

## 3. Homework

* 学习和使用教程中的代码，使用下面的数据，练习Random Forest，在training set上训练，在test set上预测，汇报模型的prediction performance: 包括 accuracy, sensitivity, specificity, roc\_auc等指标，绘制ROC曲线。

> **作业要求** ：上交一个文档汇报prediction performance，并解释如上指标所代表的意义，附ROC曲线并解释其意义。

**Helps:**

We use another R’s built-in dataset, `mtcars`, you need to run the following code to construct data:

> ```r
> df2 <- mtcars
> df2$Transmission <- factor(ifelse(df2$am, 'manual', 'automatic'))
> df2 <- df2[ , c('disp', 'hp', 'drat', 'wt', 'qsec', 'Transmission')]
> ```

> * `am` variable stores information of transmission of the car as integer, `1` means “manual”, `0` means “automatic”. we transform it into a factor and stores it into a new variable, `Transmission`
> * then we select six columns: `disp`, `hp`, `drat`, `wt`, `qsec`, `Transmission`

```r
head(df2)
```

|  | disp | hp | drat | wt | qsec | Transmission |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **Mazda RX4** | 160 | 110 | 3.9 | 2.62 | 16.46 | manual |
| **Mazda RX4 Wag** | 160 | 110 | 3.9 | 2.875 | 17.02 | manual |
| **Datsun 710** | 108 | 93 | 3.85 | 2.32 | 18.61 | manual |
| **Hornet 4 Drive** | 258 | 110 | 3.08 | 3.215 | 19.44 | automatic |
| **Hornet Sportabout** | 360 | 175 | 3.15 | 3.44 | 17.02 | automatic |
| **Valiant** | 225 | 105 | 2.76 | 3.46 | 20.22 | automatic |

> In this data, we have five features:
>
> 1. `disp`: Displacement (cu.in.)
> 2. `hp`: Gross horsepowe
> 3. `drat`: Rear axle ratio
> 4. `wt`: Weight (1000 lbs)
> 5. `qsec`: 1/4 mile time
>
> And the response variable is `Transmission`.

## 4. More reading

* The above code refers to [this post](https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/).
* For more machine learning models, you can refer to [these scripts](https://github.com/urluzhi/scripts/tree/master/Rscript/machine_learning):
  * `random_forest.R` : Random Forest 
  * `logistic_regression.R`: Logistic Regression
  * `svm.R`: SVM
  * `plot_result.R`: Plot your training and testing performance
* Last but not the least, you can also read [_The_ `caret` _package_](http://topepo.github.io/caret), a tutorial written in GitBook.



>>>>>>> 7ebcc970dcbb0806259f20e28c63e5a73ac8725b
